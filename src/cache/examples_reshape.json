[
    {
        "layout_dets": "* Authors are ordered alphabetically by the first name.\n'https://huggingface.co/Qwen\n2https://modelscope.cn/organization/qwen\n3https://github.com/QwenLM/Qwen2\nABSTRACT\nThis report introduces the Qwen2 series, the latest addition to our large lan-\nguage models and large multimodal models. We release a comprehensive suite of\nfoundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\n across diverse benchmarks on language understanding, generation, multilingual\n proficiency, coding, mathematics, and reasoning.\nThe fagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as\na base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains \n9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,\nQwen2 demonstrates robust multilingual capabilities, proficient in approximately\n 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Rus-\nsian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility\nand global reach.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren\nZhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei\nLi, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie\nWang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,\nXiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan,\nYang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang\nGuo, and Zhihao Fan\nTo foster community innovation and accessibility, we have made the Qwen2 model\nweights openly available on Hugging Face and ModelScope?, and the supplemen-\ntary materials including example code on GitHub3. These platforms also include\nresources for quantization, fine-tuning, and deployment, facilitating a wide range\nof applications and research endeavors.\nQwen Team, Alibaba Group*\nQWEN2 TECHNICAL REPORT\n",
        "page_info": {
            "page_no": 0,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "CONTENTS\n2.1Tokenizer\n2.2  Model Architecture\nQwen2 Dense Model\n2.2.2\n Qwen2 Mixture-of-experts Model\n2.2.3\n Model Configuration\n4.1 Post-training Data .\n4.1.1 Collaborative Data Annotation\n4.1.2Automated Data Synthesis\n 4.2Supervised Fine-tuning\n4.3 Reinforcement Learning from Human Feedback\nX\n2 Tokenizer & Model\n5.1 Base Language Models\n8\n5.1.1\n Core Capabilities\n8\n5.2 Instruction-tuned Model\n12\n5.2.1  Open Benchmark Evaluation .\n12\n5.2.2\n In-house Automatic Evaluation\n14\n5.2.3\n Long Context Capabilities\n15\n5.2.4\nMultilingual Evaluation\n18\n5.2.5 Safety & Responsibility\n18\n3.1 Pre-training Data\n 3.2 Long-context Training\n4  Post-training\n3  Pre-training\n5\n5 Evaluation\n1  Introduction\n6Conclusion\n19\n1  Introduction\n2.1 Tokenizer \n6Conclusion\n19\n",
        "page_info": {
            "page_no": 1,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,\ncovering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2\nincludes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathe-\nmatics content. This enrichment is hypothesized to improve reasoning abilities of LLMs. Regarding\npost-training, all models underwent supervised fine-tuning and direct preference optimization (DPO,\nRafailov et al., 2023), aligning them with human preferences through learning from human feedback.\nThis process endows the models with the capability to follow instructions effectively.\nOver recent months, we have successively introduced the Qwen series (Bai et al., 2023a) and\n progressed to Qwen1.5 (Qwen Team, 2024a). In the meantime, we have unveiled the vision-language\nmodel Qwen-VL (Bai et al., 2023b), and launched the audio-language model Qwen-Audio (Chu\n et al., 2023). In this work, we introduce the newest addition to the Qwen family of large language\nmodels and large multimodal modles: Qwen2. Qwen2 is a series of LLMs, grounded in the\nTransformer architecture (Vaswani et al., 2017), trained using next-token prediction. The model\nseries encompasses foundational, i.e., base language models, pre-trained but unaligned to human\n preferences, and instruction-tuned models, fine-tuned with single-turn and multi-turn instruction-\nfollowing datasets suitable for chat and agent purposes. Our release comprises four dense models\nwith parameter counts of 0.5 billion, 1.5 billion, 7 billion, and 72 billion, plus a Mixture-of-Experts\n(MoE) model with 57 billion parameters, of which 14 billion are activated for each token. The smaller\nmodels, specifically Qwen2-0.5B and Qwen2-1.5B, are designed for easy deployment on portable\ndevices such as smartphones, earphones, and smart glasses. Conversely, the larger models cater to\ndeployment across GPUs of varying scales.\n2TOKENIZER & MODEL\nWe have conducted a thorough evaluation of Qwen2, alongside a selection of baseline models includ-\n ing both open-weight and proprietary models accessible via API. Qwen2 outperforms competing\nmodels in evaluations of both fundamental language capabilities and instruction-tuned functionalities\nSpecifically, Qwen2-72B-Instruct, our instruction-tuned variant, scores 9.1 on MT-Bench (Zheng\net al., 2023), 48.1 on Arena-Hard (Chiang et al., 2024), and 35.7 on LiveCodeBench (Jain et al.,\n2024). Meanwhile, Qwen2-72B, the base language model, achieves 84.2 on MMLU (Hendrycks\net al., 2021a), 37.9 on GPQA (Rein et al., 2023), 64.6 on HumanEval (Chen et al., 2021), 89.5 on\nGSM8K (Cobbe et al., 2021), and 82.4 on BBH (Suzgun et al., 2023).\n2.1TOKENIZER\nFollowing the emergence of ChatGPT (OpenAI, 2022), enthusiasm for large language models\n(LLMs) has escalated globally. The release of the Llama series (Touvron et al., 2023) has further\nignited interests within the open-source community, particularly regarding GPT-level local LLMs.\nRecently, Claude-3 Opus (Anthropic, 2024) and GPT-4o (omni) (OpenAI, 2024), the updated model\nfor ChatGPT, have ascended to the pinnacle of the Chatbot Arena (Chiang et al., 2024) in quick\nsuccession. This platform is well-regarded for its human evaluations of LLMs. Moreover, Llama-\n 3 (AI@ Meta, 2024) has emerged as the state-of-the-art open-weight model series, narrowing the\n performance gap with leading proprietary models and widely acknowledged as GPT-4-level. An\n increasing number of competitive LLMs are now pursuing advancements similar to those made by the\n GPT series from OpenAI. Many of these models, including Qwen (Bai et al., 2023a), Mistral (Jiang\net al., 2023a), Gemma (Mesnard et al., 2024), etc., have been released in an open-weight manner.\nThis section introduces the tokenizer and model design of Qwen2. We detail the model architecture\nand configurations for different model sizes.\nModels of all sizes employ a common vocabulary consisting of 151,643 regular tokens and 3 control\ntokens. For more information, please refer to Bai et al. (2023a). It should be noted that, owing to\nconsiderations in distributed training, the effective size for the embeddings is larger.\nFollowing Qwen (Bai et al., 2023a), we employ the identical tokenizer based on byte-level byte-\npair encoding. Notably, this tokenizer exhibits high encoding efficiency, as evidenced by its better\ncompression rate relative to alternatives, facilitating the multilingual capabilities of Qwen2.\nINTRODUCTION\n",
        "page_info": {
            "page_no": 2,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "\\begin{array}{l}{{\\displaystyle{\\bf p}=\\mathrm{softmax}\\left(G\\left({\\bf x}\\right)\\right),}}\\\\ {{\\displaystyle{\\bf y}=\\sum_{i\\in\\mathrm{top}_{k}\\left({\\bf p}\\right)}{\\bf p}_{i}E_{i}({\\bf x}).}}\\end{array}\nE_{i}\nG\nn\nExpert Granularity The key structural difference between MoE models and dense models is\nthat MoE layers incorporate multiple FFNs, each serving as an individual expert. Consequently,\none straightforward strategy to transition from a dense architecture to an MoE architecture is to set\n the parameters of each expert equal to those of a single FFN from the original dense model. For\n example, transitioning from Mistral-7B (Jiang et al., 2023a) to Mixtral 8x7B (Jiang et al., 2024),\ninvolves activating two of the eight experts at a time. Differently, our model employs fine-grained\nexperts (Dai et al., 2024), creating smaller-scale experts while activating a greater number of experts\nsimultaneously. Given an equal total number of expert parameters and activated parameters, fine-\n grained experts offer a richer set of expert combinations. By leveraging these fine-grained experts,\nQwen2 MoE facilitates more diverse and dynamic expert utilization, thereby enhancing overall\nperformance and adaptability.\n2.2 MODEL ARCHITECTURE\nThe architecture of the Qwen2 dense models comprises multiple Transformer layers, each equipped\nwith causal attention mechanisms and feed-forward neural networks (FFNs). Key differences from\nQwen are described below:\nExpert Routing  The design of expert routing mechanisms is crucial for enhancing the performance\nof MoE models. Recently, there has been a notable trend towards integrating both shared and\nrouting-specific experts within MoE layers (Rajbhandari et al., 2022; Dai et al., 2024). We adopt this\n approach, as it facilitates the application of shared experts across various tasks while reserving others\nfor selective use in specific routing scenarios. The introduction of shared and specialized experts\noffers a more adaptable and efficient method for developing MoE routing mechanisms.\nThe Qwen2 series fundamentally constitute large language models based on the Transformer ar-\nchitecture, featuring self-attention with causal masks (Vaswani et al., 2017). Specifically, this\nseries encompasses dense language models of 4 scales and a Mixture-of-Experts (MoE) model. We\nintroduce the specifics of the dense models before delving into the MoE model's distinctive attributes.\nThe architecture of Qwen2 MoE models closely mirrors that of Qwen1.5-MoE-A2.7B (Qwen Team,\nassigned by a gated network G:\nas an expert. Each token is directed to a specific expert\nfor computation based on probabilities\n2024c). As a substitute for the original FFN, the MoE FFN consists of\nindividual FFNs, each serving\n2.2.1 QWEN2 DENSE MODEL\nGrouped Query Attention We adopt Grouped Query Attention (GQA, Ainslie et al., 2023) instead\nof conventional multi-head attention (MHA). GQA optimizes KV cache usage during inference,\nsignificantly enhancing throughput. Detailed KV head configurations for various model sizes are\nreported in Section 2.2.3.\nMoreover, we follow Qwen with the usage of SwiGLU (Dauphin et al., 2017) for activation, Rotary\nPositional Embeddings (RoPE, Su et al., 2024) for positional embedding, QKV bias (Su, 2023) for\n attention, RMSNorm (Jiang et al., 2023b) and pre-normalization for training stability.\nDual Chunk Attention with YARN To expand the context window of Qwen2, we implement Dual\nChunk Attention (DCA, An et al., 2024), which segments long sequences into chunks of manageable\nlengths. If the input can be handled in a chunk, DCA produces the same result as the original\nattention. Otherwise, DCA facilitates effective capture of relative positional information between\ntokens within and across chunks, thereby improving long context performance. Moreover, we also\nemploy YARN (Peng et al., 2023) to rescale the attention weights for better length extrapolation.\nIn the following, we present critical design considerations of Qwen2 MoE.\n2.2.2 QWEN2 MIXTURE-OF-EXPERTS MODEL\n",
        "page_info": {
            "page_no": 3,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "\\lceil n\\!\\times\\!h_{\\mathrm{E}}\\big/h_{\\mathrm{FN}}\\big]\nh_{\\mathrm{FFN}}\n50\\%\nh_{\\mathrm{E}}\nn\nExpert Initialization We initialize the experts in a similar way to upcycling (Komatsuzaki et al.,\n2023), leveraging the weights of a dense model. In contrast, our approach emphasizes diversification\namong fine-grained experts to enhance the model's representational breadth. Given the designated\n of experts while accommodating any arbitrary expert intermediate size. To promote diversity within\neach FFN copy, parameters are shuffed along the intermediate dimension. This guarantees that each\nfine-grained expert exhibits unique characteristics, even across different FFN copies. Subsequently,\nthese experts are extracted from the FFN copies, and the remaining dimensions are discarded. For\nadditional stochasticity into expert initialization, potentially enhancing the model's capacity for\n exploration during training.\n FFN is replicated\ntimes. This replication ensures compatibility with the specified number\n, the\neach fine-grained expert,\n of its parameters are randomly reinitialized. This process introduces\nexpert intermediate size\n, the number of experts\n, and the original FFN intermediate size\nThe Qwen2 series consists of models of 5 sizes, which are Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B,\nQwen2-57B-A14B, and Qwen2-72B. Table 1 lists the hyper-parameters and important information,\ne.g., the number of pre-trained tokens. Particularly, Qwen2-57B-A14B is upscaled from Qwen2-7B.\nNotably, Qwen2 models demonstrate a substantially lower Key-Value (KV) size per token relative\nto Qwen1.5 models. This characteristic translates into a reduced memory footprint, particularly\nadvantageous in long-context inference tasks.\nThe pre-training of the Qwen2 models involves the development of a new, large-scale, high-quality\nQwen and Qwen1.5 models (Bai et al., 2023a; Qwen Team, 2024a), enhancing the scale, quality, and\ndiversity of the pre-training data in several key areas:\n3PRE-TRAINING\nQuality Enhancement  The filtering algorithm has been refined with additional heuristic and model-\nbased methods, including the use of the Qwen models to filter out low-quality data. Moreover, these\n models are utilized to synthesize high-quality pre-training data.\n In the pre-training of Qwen2, our efforts were focused on refining the dataset and investigating\n methods to handle extended context lengths effectively.\n3.1 PRE-TRAINING DATA\nIn the following, we provide the key configuration and information for the Qwen2 series.\n2.2.3 MODEL CONFIGURATION\nTable 1: Architecture of Qwen2 dense and MoE models. For MoE models, 57B-A14B denotes that\nthe model has 57B parameters in total and for each token 14B parameters are active, the Intermediate\nsize denotes that of each expert, and # Activated Experts excludes the shared experts.\n",
        "page_info": {
            "page_no": 4,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "\\mathcal{D}=\\{(x_{i},y_{i})\\}\n(y_{i}^{+},y_{i}^{-})\n\\mathcal{P}=\\{(x_{i},y_{i}^{+},y_{i}^{-})\\}\ny_{i}^{+}\ny_{i}^{+}\ny_{i}^{-}\ny_{i}^{-}\nx_{i}\ny_{i}\ny_{i}\n\\mathcal{P}\nx_{i}\n\\mathcal{D}\nBased on these enhancements, the pre-training data was expanded from 3 trillion tokens in\nQwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold\nresulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a\nsignificant performance improvement over the 7 trillion token model. It is suspected that increasing\n the volume of data does not necessarily benefit model pre-training. Considering training costs, we\nopted to use the higher-quality 7 trillion token dataset for training larger models, leaving further\nexploration for future model iterations.\nFollowing extensive large-scale pre-training, we engage in a post-training phase for Qwen2. This\n process is pivotal in enhancing its proficiency across a broad spectrum of domains, including coding,\nmathematics, logical reasoning, instruction following, and multilingual comprehension. Moreover,\nit ensures that the generation from the models is in harmony with human values, making it helpful,\nhonest, and harmless. Unlike traditional methods that heavily rely on extensive human supervision,\nour approach focuses on scalable alignment with minimal human annotation (Cao et al., 2024).\nSpecifically, we investigate methods to acquire high-quality demonstration and preference data for \n Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming\nto minimize the need for human labeling while maximizing the quality and reliability of the data.\nDistribution Improvement  To ensure the model learns the distribution akin to human-like learning,\nwe conduct experiments on scaled-down models to optimize the mixing of data from various sources\nanddomains.\nTo enhance the long-context capability of Qwen2, we augmented the context length from 4,096 tokens\nto 32,768 tokens during the concluding phase of pre-training. This expansion was complemented by\nthe introduction of a significantly increased volume of high-quality, lengthy data. In conjunction with\nthese enhancements, we modified the base frequency of RoPE from 10,000 to 1,000,000 to optimize\nperformance in long-context scenarios (Xiong et al., 2023).\nThe construction of training data entails a two-step process: collaborative data annotation and\nautomated data synthesis. First, we extract the data ontology from large-scale instruction corpora,\nleading to a broad and diverse set of high-quality instructions. These instructions are systematically\nenhanced to incorporate greater complexity. Through human annotation, we obtain the target response\n. Subsequently, a variety of automated\n and their positive and negative counterparts\nAll Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of\nover 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset. The MoE\nmodel received an additional 4.5 trillion tokens of pre-training, in line with the principle of upcycling.\nSimilar to previous Qwen models, high-quality multi-task instruction data is integrated into the\nQwen2 pre-training process to enhance in-context learning and instruction-following abilities.\n The post-training data primarily consists of two components: demonstration data\nand\n preference data\nresponse, and\nand\nbeing the preferred choice over\n. The\n, where\nrepresents the instruction,\nrepresents a satisfactory\nis employed in RLHF.\nare two responses to\n, with\nset\nis utilized in SFT, whereas\nData Expansion  Compared to Qwen1.5 (Qwen Team, 2024a), we have collected a significantly\nlarger volume of high-quality code, mathematics, and multilingual data, enhancing the model's capa-\nChinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, and Vietnamese.\n4POST-TRAINING\n To fully leverage the model's length extrapolation potential, we adopted the YARN mechanism (Peng\net al., 2023) and the Dual Chunk Attention mechanism (An et al., 2024). These strategies enable\nthe model to process sequences of up to 131,072 tokens while maintaining high performance, as\nevidenced by minimal perplexity degradation in preliminary experiments.\n4.1 POST-TRAINING DATA\n3.2 LONG-CONTEXT TRAINING\n3.2 LONG-CONTEXT TRAINING\n",
        "page_info": {
            "page_no": 5,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "Data Repurposing  Creating skilled responses in literary writing tasks is challenging for annotators\nwithout specialized training. To tackle this problem, we aggregate high-quality literary works\nfrom the public domain and employ LLMs to develop instructions with varying levels of detail.\nThese instructions, paired with the original works, serve as demonstration data. For example, to\ncompile roleplay data with vivid and engaging responses, we source detailed character profiles from\n knowledge repositories such as Wikipedia and instruct LLMs to generate corresponding instructions\nand responses (Lu et al., 2024b). This process, similar to a reading comprehension task, ensures that \n the integrity of the character's profile is maintained.\n Constitutional Feedback  Constitutional AI refers to the process of guiding LLMs to generate\nresponses based on predefined sets of principles (Bai et al., 2022). To ensure adherence to guidelines\nsuch as safety and values, a constitution dataset was compiled. This dataset delineates principles to\nbe followed and those to be avoided. It was used to instruct LLMs to produce responses that either\n are aligned with or deviated from these guidelines, serving as a reference for demonstration and\npreferencedata.\nInstruction Selection  Each instruction, with tags annotated, is evaluated for tag diversity, semantic\nrichness, complexity, and intent completeness. Based on these criteria, we select a set of representative\ninstructions (Dong et al., 2023).\nHuman Annotation Multiple responses to an instruction are obtained using diverse generation\nstrategies and Qwen models of different scales. Annotators rank these responses based on their\n preferences, ensuring the best response meets established criteria, yielding both demonstration and\npreferencedata.\nExecution Feedback  For coding tasks, LLMs are employed to generate solutions and associated\ntest cases. The efficacy of these solutions is evaluated by compiling and executing them against the\n test cases, thereby creating demonstration and preference data. This methodology is also applicable\n to assessing instruction following (Dong et al., 2024). For each instruction with constraints, e.g.,\n length limit, the LLM is tasked to generate a Python verification function to ensure the response\naligns with the instruction requirements.\nInstruction Evolution  To enrich the instruction dataset, a self-evolution strategy (Zhao et al., 2024)\nis employed, prompting the Qwen models to add constraints or requirements to existing instructions,\n thereby increasing their complexity and ensuring a diverse range of difficulty levels within the dataset.\nMaintaining the quality of annotations for responses to instructions presents significant challenges on\na large scale, particularly those that require expertise, experience, carefulness, or patience. To address\n these challenges, we devised various automated alignment strategies to synthesize data at scale.\nAutomatic Ontology Extraction  The process initiates with the application of InsTag (Lu et al.,\n2024c), an open-set fine-grained tagger, to extract the underlying ontology from a large-scale\n instruction dataset. Subsequent manual refinement ensures the accuracy of the extracted ontology.\nalignment strategies are employed to synthesize a substantial volume of artificially annotated data\nacross the domains of code, mathematics, instruction-following, creation, role-playing, and safety.\nRejection Sampling For mathematical or similar tasks with definitive final answers, rejection\nsampling (Yuan et al., 2023) is applied to improve the quality of solutions. Large language models\n(LLMs) are tasked to generate multiple responses, namely the reasoning paths, for each instruction.\nPaths that result in accurate conclusions and are considered reasonable by the model are preserved,\nserving as demonstration data. Preference data is generated by contrasting correct and incorrect paths.\n4.1.1 COLLABORATIVE DATA ANNOTATION\n4.1.2 AUTOMATED DATA SYNTHESIS\n",
        "page_info": {
            "page_no": 6,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "7\\times10^{-6}\n\\mathit{\\bar{y}_{i}^{+}}\n7\\times10^{-7}\ny_{i}^{-}\n\\mathrm{C++}\n\\mathcal{P}\nOur training regime for RLHF comprises two sequential stages: offline and online training. In the\n the online training stage, the model iteratively refines its performance in real-time, leveraging reward\nmodels for immediate feedback. Specifically, we sample multiple responses from the current policy\n model, and the reward model selects the most and the least preferred responses, forming preference\npairs that are used for DPO in each episode. Moreover, we employ Online Merging Optimizer (Lu\n et al., 2024a) to mitigate the alignment tax, i.e., the performance degradation associated with aligning\nmodel generation with human preferences.\nlikelihood between\nand\nwith Direct Preference Optimization (DPO, Rafailov et al., 2023). In\n offline training stage, we use a pre-compiled preference dataset\nto maximize the difference in\nTo thoroughly assess the Qwen2 models, consisting of both base and instruction-tuned models,\nwe implement a comprehensive evaluation protocol. This protocol examines a range of compe-\ntencies, including general knowledge understanding, language comprehension, generation, coding,\nmathematics, reasoning, and additional areas of expertise. Specifically, base models are assessed\nusing established benchmark datasets for large language models (LLMs), with responses elicited\n through few-shot prompting, unless specified otherwise. For instruction-tuned models, in addition to\nbenchmark evaluations, we prioritize human preference assessments.\nWe have assembled an extensive instruction dataset featuring more than 50o,000 examples that\n cover skills such as instruction following, coding, mathematics, logical reasoning, role-playing,\n multilingualism, and safety. Our model was fine-tuned for two epochs with a sequence length of\nmaximum value of 1.0.\n 32,768 tokens. To optimize learning, the learning rate was gradually decreased from\nto\n. To address overfitting, we applied a weight decay of 0.1 and gradients were clipped at a\n4.3  REINFORCEMENT LEARNING FROM HUMAN FEEDBACK\n4.2 SUPERVISED FINE-TUNING\nIn this section, we illustrate the evaluation of the base language models of the Qwen2 series. Specifi-\ncally, we evaluate the models on benchmark datasets for knowledge and basic capabilities and apply\n multilingual benchmark datasets to evaluate their support of languages. As there are multiple model\nsizes, we compare them with the state-of-the-art (SOTA) models of similar or larger sizes.\nBenchmarks and Evaluation Protocol The common practice of evaluating the core capabilities\nof base language models is the implementation of benchmark dataset evaluation with few-shot or\nzero-shot prompting. The evaluation mainly focuses on the model performance of natural language\nunderstanding, general question answering, coding, mathematics, scientific knowledge, reasoning, etc.\nThe datasets for evaluation include MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang\net al., 2024) (5-shot), GPQA (Rein et al., 2023) (5shot), Theorem QA (Chen et al., 2023a) (5-shot),\nBBH (Suzgun et al., 2023) (3-shot), HellaSwag (Zellers et al., 2019) (10-shot), Winogrande (Sak-\naguchi et al., 2021) (5-shot), TruthfulQA (Lin et al., 2022a) (0-shot), ARC-C (Clark et al., 2018)\n(25-shot), HumanEval (Chen et al., 2021) (0-shot), MBPP (Austin et al., 2021) (0-shot), EvalPlus(Liu\nScript, C#, Bash, and JavaScript), GSM8K (Cobbe et al., 2021) (5-shot), MATH (Hendrycks et al.,\n2021b) (4-shot), C-Eval (Huang et al., 2023) (5-shot), and CMMLU (Li et al., 2023) (5-shot). Multi-\nlingual datasets can be grouped into four categories: (a) Exam: M3Exam (5-shot, we only choose\nexamples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova\net al., 2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French,\nPortuguese, German, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar\net al., 2023) (5-shot), XCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023)\n(5-shot), XStoryCloze (Lin et al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c)\net al., 2023a) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot on Python,\n,Java,PHP,Type-\n5.1.1 CORE CAPABILITIES\n5.1 BASE LANGUAGE MODELS\n5EVALUATION\n",
        "page_info": {
            "page_no": 7,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "{\\bf70B+}\n8\\mathrm{x}22\\mathrm{B}\n@\n\\cdot8x22\\mathrm{B}\nQwen2-72B In terms of the largest model of Qwen2, we compare Qwen2-72B with competitive\n2024), as well as Qwen1.5-72B (Qwen Team, 2024a) and Qwen1.5-110B (Qwen Team, 2024b).\nThe results are reported in Table 2. Qwen2-72B outperforms Llama-3-70B in general knowledge\nunderstanding on both MMLU and MMLU-Pro, achieving accuracy improvements of 4.7 and 2.8,\nrespectively. In scientific assessments, Qwen2-72B demonstrates superiority over Llama-3-70B with\nenhancements of 1.6 and 9.8 on GPQA and Theorem QA. Upon enrichment of coding data, Qwen2-\n 72B exhibits a significant 18.3 and 10.0 percentage point advantage over Qwen1.5-72B in HumanEval\nand MBPP evaluations. Enhanced mathematics-related data allows Qwen2-72B to outperform\nQwen1.5-72B by 10.0 and 17.0 percentage points in the GSM8K and MATH benchmarks. Qwen2-\n72B displays reasoning capabilities equivalent to Llama-3-70B, considering BBH, Winogrande,\n and ARC-C, attributable to its improved coding and mathematical data. In assessing language\nand also outperforms Qwen1.5-72B.\nbaseline open-weight models, including Mixtral-8x22B (Jiang et al., 2024), Llama-3-70B (AI\nMeta,\nunderstanding in Chinese, Qwen2-72B significantly outperforms Mixtral-\nand Llama-3-70B,\nMathematics: MGSM (Goyal et al., 2022) (8-shot CoT); and (d) Translation: Flores-101 (Goyal et al.\n2022) (5-shot).\ndemonstrates advantages over the baselines.\nTable 2:Performance of the\nmodels. We compare Qwen2-72B with the baselines, including\nMixtral-\n, Llama-3-70B, Qwen1.5-110B, and Qwen1.5-72B. For most datasets, Qwen2-72B\nQwen2-57B-A14B For the evaluation of the MoE model, Qwen2-57B-A14B is compared against\nbaselines of similar sizes. These baselines include other MoE models, such as Mixtral-8x7B (Jiang\net al., 2024) and Jamba (Lieber et al., 2024), and dense models, such as Yi-1.5-34B (Young et al., 2024)\n",
        "page_info": {
            "page_no": 8,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "\\mathbf{30B+}\n\\bf{40B+M o E}\n{\\cdot}8\\mathrm{x}7\\mathrm{B}\nand Qwen1.5-32B (Qwen Team, 2024a), both of which have approximately 30 billion parameters\nThe results are shown in Table 3. We anticipate that Qwen2-57B-A14B, which activates 14 billion\nparameters, will match the performance of a 30 billion parameter dense equivalent Qwen2 model. Our\n evaluation reveals that Qwen2-57B-A14B performs comparably to Yi-1.5-34B in natural language\nunderstanding tasks. Moreover, it outperforms the baseline models in coding and mathematics tasks.\nAdditionally, Qwen2-57B-A14B demonstrates robust Chinese language understanding capabilities,\nrivaling the larger Qwen2-72B model. In essence, Qwen2-57B-A14B is an efficient model that, while\nactivating only 14 billion parameters per forward pass, maintains the performance level of a 30 billion\nparameter densemodel.\nQwen2-7B  The 7B model is widely utilized, as it enables the execution in 16-bit foating points on\naccelerators equipped with 16GB memory. Our focus is on comparing this model with other leading\n7B models, including Llama-3-8B, which has recently demonstrated exceptional performance in the\nChatbot Arena (Chiang et al., 2024). This comparison also includes Mistral-7B-v0.2 (Jiang et al.\n2023a), Gemma-7B (Mesnard et al., 2024), and our predecessor, Qwen1.5-7B (Qwen Team, 2024a).\n10\nMoE model with a total of 57 billion parameters and 14 billion activated parameters, is designed\nto match the performance of 30 billion parameter dense models. This comparison includes dense\nResults demonstrate that Qwen2-57B-A14B achieves competitive performance overall, with a notable\nsuperiority in coding and mathematics tasks.\nTable 3: Performance of the\ndensemodels and\nmodels. Qwen2-57B-A14B, an\nmodel baselines: Yi-1.5-34B and Qwen1.5-32B, as well as MoE baselines: Mixtral.\nandJamba.\n",
        "page_info": {
            "page_no": 9,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "^{7\\mathrm{B}+}\n{\\bf7B+}\nQwen2-1.5B & Qwen2-0.5B  To evaluate the performance of our smaller models, specifically\nQwen2-1.5B and Qwen2-0.5B, we compare them against established baselines: Phi-2 (Abdin et al.,\n2024), Gemma-2B (Mesnard et al., 2024), and Qwen1.5-1.8B (Qwen Team, 2024a). The results\n are given in Table 5. In language understanding, Qwen2-1.5B outperforms Phi-2, a model trained\non textbook-like data. For coding tasks, Qwen2-0.5B matches the performance of Gemma-2B and\nQwen1.5-1.8B, while Qwen2-1.5B surpasses these baselines, except for Phi-2. Both Qwen2 models\nexhibit superior performance in mathematics compared to their competitors. In terms of general\nreasoning, we find that Phi-2 generally outperforms all others, which to some extent reflects the\nsignificance of textbook data for reasoning capabilities. In TruthfulQA, Qwen2-1.5B performs the\nbest, demonstrating that smaller models does not necessarily suffer from hallucination. In Chinese\nlanguage understanding, both Qwen2 models outperform all the others, a trend consistent with larger\nmodels in their respective comparisons.\n11\nIn general, the Qwen2 series demonstrates superior performance against the baselines across different\nmodel sizes. Notably, Qwen2-72B exhibits the highest performance among all Qwen2 models,\nunderscoring the eficacy of model size scaling.\nThe results can be found in Table 4. Qwen2-7B demonstrates superior performance across most\ndatasets compared to other models, particularly excelling in coding tasks, mathematics, and Chinese\nlanguage tasks. It also shows strong performance in multilingual understanding and exams. This\n indicates that Qwen2-7B has been optimized for a wide range of language and logic-based tasks,\nshowcasing its versatility and advanced capabilities.\nQwen2-7B demonstrates significant advantages over the baselines in most of the evaluation datasets.\nthe-art\nmodels including Mixtral-7B, Gemma-7B, Llama-3-8B, and our previous Qwen1.5-7B.\nTable 4: Performance of the\nmodels. We compare Qwen2-7B with previously released state-of-\n",
        "page_info": {
            "page_no": 10,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "\\cdot8x22\\mathrm{B}\n2023)^{4}\nTo comprehensively evaluate the quality of instruction-tuned models, we compile automatic and\nhuman evaluation to assess the capabilities and human preference. For the evaluation of basic\n capabilities, we apply similar datasets in the pre-trained model evaluation, which target on natural\nlanguage understanding, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU,\nMMLU-Pro, GPQA, and Theorem QA for language understanding and knowledge, HumanEval,\nMBPP, MultiPL-E, and LiveCodeBench v1 (Jain et al., 2024) for coding, GSM8K and MATH for\nmathematics. Additionally, we assess the performance of human preference alignment and instruction\n following by evaluating on benchmarks including MT-Bench (Zheng et al., 2023), Arena-Hard (Li\net al., 2024), AlignBench (Liu et al., 2023b), MixEval (Ni et al., 2024) whose results approximate \n those of Chatbot Arena, and IFEval (Zhou et al.,\nfor instruction following.\nQwen2-72B-Instruct\nWe compare Qwen2-72B-Instruct against the instruction-tuned models in-\npresented in Table 6. It can be found that a strong base language model can help boost the downstream\n performance of the instruction-tuned model. Specifically, Qwen2-72B-Instruct outshines its peers in\n areas such as language understanding, coding, and mathematics, with the exception of GPQA and\nMBPP. Regarding human preference alignment and instruction following, Qwen2-72B has significant\n advantages over the baselines. We assume this achievement is attributed to both the high-quality\npre-trained model and improvements in both data and training techniques for post-training.\ncludingMixtral-\n-Instruct, Llama-3-70B-Instruct, as well as Qwen1.5-72B-Chat. The results are\nTo critically evaluate instruction-tuned models, we implement a multifaceted approach. Assessments\nof foundational skills and human preferences are conducted using open datasets and benchmarks. Our\ndetailed in-house examinations further probe model competencies in key areas. A particular focus is\nplaced on assessing long context capability. Safety measures include multilingual safety assessments\n and red teaming exercises. The following sections detail the evaluation methods and their outcomes.\n5.2 INSTRUCTION-TUNED MODEL\n5.2.1 OPEN BENCHMARK EVALUATION\n4For simplicity, we report the results of the subset strict-prom.\nTable 5: Performance of the smaller models. We compare our Qwen2-0.5B and Qwen2-1.5B\nwith the previous SOTA small models including Phi-2, Gemma-2B and Qwen1.5-1.8B. Qwen2-0.5B\nwith a much smaller model size achieves competitive performance, and Qwen2-1.5B significantly\noutperforms Qwen2-0.5B.\n12\n",
        "page_info": {
            "page_no": 11,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "{\\bf70B+}\n\\cdot8x22\\mathrm{B}\nQwen2-7B-Instruct Within the spectrum of 7B to 9B models, we compare Qwen2-7B-Instruct\nwith Llama-3-8B-Instruct, Yi-1.5-9B-Chat, GLM-4-9B-Chat, and Qwen1.5-7B-Chat. The results\ncan be found in Table 8. Qwen2-7B-Instruct demonstrates substantial advancements compared\nto its predecessor, Qwen1.5-7B-Chat, across comprehensive evaluations, notably achieving higher\nscores in coding and mathematics-related tasks. Compared with the recent SOTA model, Llama-3-\n8B-Instruct, Qwen2-7B-Instruct demonstrates competitive performance and specifically it achieves\n superior performance in coding. Nonetheless, in terms of instruction following, Qwen2-7B-Instruct\n greatly falls behind the competitor. To address this limitation, we plan to augment the 7B model's\ninstruction-following ability by enhancing the quality of post-training data, ensuring a more robust\nunderstanding and execution of complex commands.\nQwen2-1.5B-Instruct & Qwen2-0.5B-Instruct  In the context of smaller models, we compare\nQwen2-0.5B-Instruct with Qwen1.5-0.5B-Chat, and Qwen2-1.5B-Instruct with Qwen1.5-1.8B-Chat.\n Notably, the complexity of certain datasets designed for larger models exceeds the capabilities of\nthese smaller models; thus, our analysis focuses on a selected subset. As detailed in Table 9, the\nQwen2 models demonstrate a marked advantage over their predecessors in both core capabilities and\ninstruction-following tasks. The achievement mainly attributes to the scaling of pre-training data.\nConsequently, our results affirm that data scaling remains an effective strategy for enhancing model\nperformance, even in the domain of sub-billion parameter models.\nQwen2-57B-A14B-Instruct For medium-size models, we compare Qwen2-57B-A14B-Instruct\nwith Mixtral-8x7B-Instruct, another MoE baseline, as well as the dense SOTA models with over 30\nbillion parameters, e.g., Yi-1.5-34B-Chat and Qwen1.5-32B-Chat. The results are provided in Table 7.\nCompared with Qwen1.5-32B-Chat, Qwen2-57B-A14B-Instruct reaches superior performance in\nalmost all benchmarks, and compared with the 30B SOTA model Yi-1.5-34B-Chat, Qwen2-57B-\nA14B-Instruct has gained advantages in most evaluations except for those for mathematics. In terms\nof the evaluation for alignment, the advantages of Qwen2-57B-A14B-Instruct are notably evident.\nInstruct’ or “-Chat\" is omitted in the table. Qwen2-72B-Instruct demonstrates advantages in core\ncapabilities, and superior performance in human preference alignment.\nTable 6: Performance of\ninstruction-tuned models. We compare Qwen2-72B-Instruct with\nMixtral-\n-Instruct, Llama-3-70B-Instruct, Qwen1.5-72B-Chat, and Qwen1.5-110B-Chat. “\n13\n",
        "page_info": {
            "page_no": 12,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "^{\\mathbf{30B+}}\n{\\bf40B+}\nChinese Evaluation For the evaluations in Chinese, we focus on comparing the performance of\nQwen2 models with the Qwen1.5 counterparts. For the small models, Qwen2-1.5B-Instruct generally\noutperforms Qwen1.5-1.8B-Chat in almost all the evaluations even with fewer parameters. In terms of\nthe comparison of 7B models, the advantages of Qwen2 are more significant. Noteworthy is Qwen2-\n72B's superior performance to Qwen1.5-110B-Chat, despite the latter's greatly more parameters.\nThe MoE model displays superior performance across most domains relative to Qwen1.5-32B-Chat,\nexcluding knowledge understanding. This discrepancy may be attributed to a short of pre-training\ntokens. In the near future, we are about to continue the pre-training of the MoE model to discover its \nscalingbehaviors.\nDespite a number of open benchmark datasets for the evaluation, we believe that it is far from\nsufficient to fully comprehend the capabilities of LLMs. Specifically, we have made a series of\n in-house datasets that assess different capabilities of the models, e.g., knowledge understanding,\ntext generation, coding, etc. The evaluation is in Chinese and English. The results are gathered in\nTable 10 and Table 11, respectively.\nQwen2-57B-A14B-Instruct with the similar-size MoE model Mixtral-8x7B-Instruct, 30B dense\nmodels such as Yi-1.5-34B-Chat and Qwen1.5-32B-Chat. “-Instruct\"’ or “-Chat\" is omitted in the\ntable. Qwen2-57B-A14B-Instruct is competitive with the recent SOTA 30B dense models, and\nsignificantly outcompetes the MoE baseline.\nTable 7: Performance of\ndenseand\nMoE instruction-tuned models. We compare\n14\nEnglish Evaluation  For English, we compare Qwen2 with both Qwen1.5 and Llama-3. Similarly,\nthe small models of Qwen2 significantly outcompete the Qwen1.5 counterparts. However, in com-\n parison with Llama-3-70B, Qwen2-72B-Instruct is falling behind by small margins especially in\ncomprehension and coding. We assume both the amount of English tokens for pre-training and the\nquantity and diversity of data for post-training lead to the performance gap in English.\n5.2.2 IN-HOUSE AUTOMATIC EVALUATION\n",
        "page_info": {
            "page_no": 13,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "10\\%\n{\\bf7B+}\n0\\%\n Needle in a Haystack  This experiment assesses a model's proficiency in pinpointing facts within\nvoluminous texts. Texts with 8K, 16K, .., 128K tokens in length were crafted, with facts strategically\n For contexts over 32K, YARN (Peng et al., 2023) was applied in this evaluation. As illustrated in\nFigure 1, Qwen2-72B-Instruct exhibits exceptional accuracy in retrieving information from the entire\n128K context. Coupled with its inherent strength, this model emerges as the optimal choice for\nprocessing extensive texts, assuming sufficient resources are accessible. Additionally, models within\nthe same series showcases remarkable performance across different context lengths. Precisely, Qwen2-\n7B-Instruct achieves a high level of accuracy in handling contexts up to 128K tokens. Meanwhile,\nQwen2-57B-A14B-Instruct manages contexts up to 64K tokens proficiently, and the two smaller\nmodels in the Qwen2 series could support contexts of 32K tokens.\n, encompassed two instances.\n positioned at varying depths. Each depth interval, e.g., from\nto\nThree methods to evaluate long context capabilities are employed: the Needle in a Haystack (NIAH,\nKamradt, 2023), NeedleBench (OpenCompass Contributors, 2023), and LV-Eval (Yuan et al., 2024))\nrecent SOTA models with 7-9 billion parameters, including Llama-3-8B-Instruct, Yi-1.5-9B-Chat,\nGLM-4-9B-Chat, and Qwen1.5-7B-Chat. “-Instruct\" or \"-Chat’ is omitted in the table. Qwen2-7B-\nInstruct demonstrates competitive performance against Llama-3-8B-Instruct.\nTable 8: Performance of\ninstruction-tuned models. We compare Qwen2-7B-Instruct with the\n15\nTable 9: Performance of smaller instruction-tuned models. We compare both Qwen2-0.5B-Instruct\nand Qwen2-1.5B-Instruct with Qwen1.5-0.5B-Chat and Qwen2-1.8B-Chat. “-Instruct\" or “-Chat\"\nis omitted in the table. Compared with the similar-size baselines, Qwen2 significant surpasses the\nperformance of Qwen1.5.\n5.2.3 LONG CONTEXT CAPABILITIES\n",
        "page_info": {
            "page_no": 14,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "Table 10: Performances of Qwen2-Instruct models on our in-house Chinese automatic evaluation\nbenchmark. Scores of Qwen2 models surpassing their comparable-sized Qwen1.5 counterparts are\nin bold. Qwen2-57B-A14B-Instruct is compared with Qwen1.5-32B-Chat.\nTable 11: Performances of Qwen2-Instruct models on our in-house English automatic evaluation\nbenchmark. Scores of Qwen2 models surpassing their comparable-sized Qwen1.5 and Llama-3\ncounterparts are in bold. Qwen2-57B-A14B-Instruct is compared with Qwen1.5-32B-Chat.\n16\n",
        "page_info": {
            "page_no": 15,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "+Y A R N{+}D C A\n32\\mathrm{k}\n17\nFigure 1: Performance of Qwen2 instruction-tuned models on Needle in A Haystack Test. All\nmodels that supports context lengths above 32k tokens integrates the YARN mechanism.\nTable 12: Performance of Qwen2-72B-Instruct and Qwen2-7B-Instruct on NeedleBench and\nLV-Eval.\ndoes not change the model behavior within\ntokens.\nTesting Qwen2-Instruct via \"Needle in A HayStack\"\nRetrieve Facts from Given Documents across Context Lengths and Document Depth\n",
        "page_info": {
            "page_no": 16,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "For the multilingual evaluation, we implement a comprehensive human evaluation for the assessment\nof multilingual capabilities. Specifically, we design diverse test cases assessing different capabilities\n of large language models, and we have test cases that are in a number of languages. For the annotators,\nwe invite one professional annotator for each language who majors in the language for the evaluation.\nFor each test case, the annotator grades the response from model with a score from 1 to 5.\nLV-Eval LV-Eval comprises 11 diverse QA datasets that demand comprehension of multiple pieces\nof evidence at once. To rectify the shortcomings of its original metric, which was excessively stringent\nand led to a high rate of false negatives, we adopt the keyword recall as the reported score. As shown\nin Table 12, integrating YARN and DCA substantially bolsters the long-context competencies of\nQwen2 models on LV-Eval. Qwen2-7B-Instruct achieves parity with ChatGLM4-9B-1M, albeit with\n a more noticeable decline at extended contexts. Moreover, Qwen2-72B-Instruct demonstrates strong\nperformance across all lengths, confirming its proficiency in handling long-context tasks.\nWe report the results of our model and the baselines in the evaluation of different languages. From\nTable 13, it can be found that on average Qwen2-72B-Instruct significantly outperforms GPT-3.5-\nTurbo and it is competitive with GPT-4-Turbo and slightly falls behind Claude-3-Opus. This shows \nthat our multilingual pre-training and instruction tuning data contribute to the multilingual capabilities\n of Qwen2-72B-Instruct and it is competitive with most state-of-the-art proprietary LLMs.\nTable 13: Performance of Qwen2-72B-Instruct and proprietary LLMs in multilingual human\nevaluation. We compare Qwen2-72B-Instruct with GPT-3.5-Turbo-1106, GPT-4-Turbo-0409, GPT-\n4o-0513, Claude-3-Opus-0229. Scores range from 1 to 5. Overall, Qwen2-72B-Instruct performs\nsubstantially better than GPT-3.5-Turbo but there is progress to be made to be competitive with the\n proprietary models released in the last 6 months.\nLLMs with openly accessible weights effectively accelerate the development of the research as well\nas their applications. Moreover, we believe that it is crucial to build safe and responsible LLMs so\n that the effect of the misuse of AI technologies could be significantly alleviated.\n5.2.4 MULTILINGUAL EVALUATION\nNeedleBench NeedleBench ups the challenge on NIAH by including multiple facts (two to five) in\n passages, necessitating simultaneous identification and multi-hop reasoning. Table 12 reveals that\nthe integration of YARN and DCA (An et al., 2024) notably improves Qwen2 models' long-context\n abilities. Qwen2-7B-Instruct surpasses ChatGLM4-9B-1M (Zeng et al., 2024), which claims a 1M\n context length. Moreover, Qwen2-72B-Instruct demonstrates strong performance, with an accuracy\nreduction of just 6 points, compared to ChatGLM4-9B-1M, which shows a more pronounced decline\n of 11 points, particularly given its lower initial accuracy.\nWe implement a multilingual safety evaluation that tests the LLMs in different languages. Specif-\nically, we assess the safety performance of the models in the topics about illegal behaviors, fraud,\n5.2.5 SAFETY & RESPONSIBILITY\n18\n",
        "page_info": {
            "page_no": 17,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "\\cdot8x22\\mathrm{B}\n{\\bf8x22B}\n The results are presented in Table 14, where the proportion of harmful responses generated by the\n models are shown and the lower, the better. It can be observed that Qwen2-72B-Instruct performs\nbetter than the proprietary model, GPT-4, and significantly outperforms the open-weight model,\nbe a safer and more responsible model, especially in terms of pornography, which is a conventionally\ndifficult category to differentiate even for humans.\nMixtral-\n-Instruct. However, we believe that there is still much room for our model to improve to\nThis technical report has presented the Qwen2 series, a versatile suite of foundational and instruction-\ntuned language models, ranging from 0.5 to 72 billion parameters, including models of dense and\nMixture-of-Experts architecture. Qwen2 outperforms previous open-weight models, notably its \npredecessor Qwen1.5, and displays competitive performance against proprietary models across a\nbroad spectrum of benchmarks in language understanding, generation, multilingual capabilities,\ncoding, mathematics, and reasoning. In this update, we have extra focus on long-context, multi-\nlingual, coding, mathematics capabilities and safety and responsibility. In a commitment to fostering\ninnovation and accessibility within the community, we have made the Qwen2 model weights openly\naccessible, which enables researchers and developers to harness the full potential of Qwen2 in a\nvariety of applications and research projects. Through these efforts, we aim to contribute to the\nadvancement of AI technologies and their positive impact on society.\npornography, and privacy. We have collected prompts prone to jail-breaking and use them to test\nwhether the models can provide safe responses by rejection.\n6CONCLUSION\n19\nTable 14: Performance of models in safety evaluation. We compare Qwen2-72B-Instruct with\nwith risks than the competitors.\nGPT-4 andMixtral-\n-Instruct. The lower, the beter. Qwen2-72B-Instruct rejected more prompts\n",
        "page_info": {
            "page_no": 18,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "@\n Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge.\nYu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,\nChengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,\nSinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin\nXu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren\nZhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\n Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas\nJoseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from\nAI feedback. CoRR, abs/2212.08073, 2022.\nAnthropic.\nThe Claude 3 model family:\n Opus, Sonnet, Haiku.\nTechnical  re-\nport,  Anthropic， AI,  2024.\nURL\nhttps://www-cdn.anthropic.com/\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Mode1_Card_C1aude_3.pdf.\n Jacob Austin, Augustus Odena, Maxwell 1. Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with\nlarge language models. CoRR, abs/2108.07732, 2021.\nMarah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del\nGiorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann,\nYin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nMichael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp\nWitte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models,\n2024. URL https://www.microsoft.com/en-us/research/blog/phi-2-the-\nsurprising-power-of-small-language-models/.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa,\nNaman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele bench-\nmark: A parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884,\n2023.\n Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\n Zhou, and Jingren Zhou. Qwen- VL: A frontier large vision-language model with versatile abilities.\nCoRR, abs/2308.12966, 2023b.\n Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben\nHe, Xianpei Han, Le Sun, Hongyu Lin, and Bowen Yu. Towards scalable automated alignment of\nLLMs: A survey. CoRR, abs/2406.01252, 2024.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit\n Sanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints.\n In EMNLP, pp. 4895-4901. Association for Computational Linguistics, 2023.\nblob/main/MODEL_CARD.md.\nAI\nMeta. Llama 3 model card, 2024. URL https: / /github.com/meta-1lama/1lama3/\nChenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\nTraining-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\n Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Trans. Software Eng., 49(7):3675-3691, 2023.\n20\nREFERENCES\nREFERENCES\n",
        "page_info": {
            "page_no": 19,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": " Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert- Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. CoRR, abs/2107.03374, 2021.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng\nLi, Hao Zhang, Banghua Zhu, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot\narena: An open platform for evaluating LLMs by human preference. CoRR, abs/2403.04132, 2024.\n21\nZhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen,\nJunying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. Multilingual-\nSIFT: Multilingual supervised instruction fine-tuning, 2023b. URL ht tps : / /github . com/\nFreedomIntelligence/MultilingualSIFT.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and\nTony Xia. TheoremQA: A theorem-driven question answering dataset. In EMNLP, pp. 7889-7901.\nAssociation for Computational Linguistics, 2023a.\n Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and\n Jingren Zhou. Qwen-Audio: Advancing universal audio understanding via unified large-scale\naudio-language models. CoRR, abs/2311.07919, 2023.\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert- Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. CoRR, abs/2107.03374, 2021.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and\nTony Xia. TheoremQA: A theorem-driven question answering dataset. In EMNLP, pp. 7889-7901.\nAssociation for Computational Linguistics, 2023a.\nZhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen,\n Junying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. Multilingual-\nSIFT: Multilingual supervised instruction fine-tuning, 2023b. URL ht tps : / / github . com/\nFreedomIntelligence/MultilingualSIFT.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng\nLi, Hao Zhang, Banghua Zhu, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot\narena: An open platform for evaluating LLMs by human preference. CoRR, abs/2403.04132, 2024.\nYunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and\nJingren Zhou. Qwen-Audio: Advancing universal audio understanding via unified large-scale\naudio-language models. CoRR, abs/2311.07919, 2023.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge.\nCoRR, abs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\n Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui,\n and Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts\nlanguage models. CoRR, abs/2401.06066, 2024.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp.\n933-941. PMLR, 2017.\nGuanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang,\n Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected\nby supervised fine-tuning data composition. CoRR, abs/2310.05492, 2023.\nGuanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou.\n Self-play with execution feedback: Improving instruction-following capabilities of large language\nmodels. CoRR, abs/2406.13542, 2024.\n Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Al-\nbina Akhmetgareeva, Anton A. Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana\nIsaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin,\nPolina Mikhailova, Denis Dimitrov, Alexander Panchenko, and Sergey Markov. MERA: A com-\n prehensive LLM evaluation in russian. CoRR, abs/2401.04531, 2024.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana\nKrishnan, Marc'Aurelio Ranzato, Francisco Guzman, and Angela Fan. The Flores-101 evalua-\ntion benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput.\nLinguistics,10:522-538,2022.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge.\nCoRR, abs/1803.05457, 2018.\n Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\n Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\n Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui,\nand Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts\nlanguage models. CoRR, abs/2401.06066, 2024.\n",
        "page_info": {
            "page_no": 20,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "22\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm\nTransformers: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023b.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\nLélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas\nWang, Timothée Lacroix, and William E1 Sayed. Mistral 7B. CoRR, abs/2310.06825, 2023a.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\n Steinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net,\n2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In\nNeurIPS Datasets and Benchmarks, 2021b.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A\nmulti-level multi-discipline chinese evaluation suite for foundation models. In NeurIPS, 2023.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free\nevaluation of large language models for code. CoRR, abs/2403.07974, 2024.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\nLélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas\nWang, Timothée Lacroix, and William El Sayed. Mistral 7B. CoRR, abs/2310.06825, 2023a.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\nScao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\nMixtral of experts. CoRR, abs/2401.04088, 2024.\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm\nTransformers: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023b.\nGregory Kamradt. Needle in a haystack - pressure testing LLMs, 2023. URL ht tps : / / github .\nCom/ gkamradt /LLMTest_NeedleInAHaystack.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua\nAinslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-\nexperts from dense checkpoints. In ICLR. OpenReview.net, 2023.\nFajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass\nprimary school exams in Indonesia: A comprehensive test on IndoMMLU. In EMNLP, pPp.\n12359-12374. Association for Computational Linguistics, 2023.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\n Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese. CoRR,\nabs/2306.09212, 2023.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gon-\nzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and\n BenchBuilder pipeline. CoRR, abs/2406.11939, 2024.\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\nRozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A hybrid Transformer-Mamba\nlanguage model. CoRR, abs/2403.19887, 2024.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human\nfalsehoods. In ACL (1), pp. 3214-3252. Association for Computational Linguistics, 2022a.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,\nNaman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura,\nVishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T.\nDiab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language\nmodels. In EMNLP, pp. 9019-9052. Association for Computational Linguistics, 2022b\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\nScao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.\nMixtral of experts. CoRR, abs/2401.04088, 2024.\nGregory Kamradt. Needle in a haystack - pressure testing LLMs, 2023. URL ht tps : / / github .\nCom/gkamradt /LLMTest_NeedleInAHaystack.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua\nAinslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-\nexperts from dense checkpoints. In ICLR. OpenReview.net, 2023.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free\nevaluation of large language models for code. CoRR, abs/2403.07974, 2024.\nFajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass\nprimary school exams in Indonesia: A comprehensive test on IndoMMLU. In EMNLP, p.\n12359-12374. Association for Computational Linguistics, 2023.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A\nmulti-level multi-discipline chinese evaluation suite for foundation models. In NeurIPS, 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\n Steinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net,\n2021a.\n Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In\nNeurIPS Datasets andBenchmarks,2021b.\n Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. CMMLU: Measuring massive multitask language understanding in Chinese. CoRR,\nabs/2306.09212,2023.\n",
        "page_info": {
            "page_no": 21,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": " Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and\n Jingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language\nmodels. In ICLR. OpenReview.net, 2024c.\nThomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\n Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe\nSessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros,\nAmbrose Slone, Amelie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai,\nBobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George\nTucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney,\nIvan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\nStanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine\nLee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej\nMikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar\nChang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona\nComanescu, Reena Jana, Rohan Anil, Ross Mcllroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\n Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De,\nTed Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed,\nZhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff\nDean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral,\nFernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and\nKathleen Kenealy. Gemma: Open models based on Gemini research and technology. CoRR,\nabs/2403.08295,2024.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,\nEdward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning. In ACL\n(1), pp. 15991-16111. Association for Computational Linguistics, 2023.\nEdoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.\nXCOPA: A multilingual dataset for causal commonsense reasoning. In EMNLP (1), pp. 2362-2376.\nAssociation for Computational Linguistics, 2020.\n23\n Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and\nYang You. MixEval: Deriving wisdom of the crowd from LLM benchmark mixtures. CoRR,\nabs/2406.06565,2024.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nOpenCompass Contributors. OpenCompass: A universal evaluation platform for foundation models,\n2023. URL https: / /github.com/open-compass /opencompass.\n Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\n ChatGPT really correct? Rigorous evaluation of large language models for code generation. In\nNeurIPS,2023a.\nKeming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions\n of all characters: Attaining arbitrary role-play via self-alignment. CoRR, abs/2401.12474, 2024b.\nKeming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. Online merging optimizers\nfor boosting rewards and mitigating tax in alignment. CoRR, abs/2405.17931, 2024a.\n Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke,\nYifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie\nHuang, Yuxiao Dong, and Jie Tang. AlignBench: Benchmarking Chinese alignment of large\nlanguage models. CoRR, abs/2311.18743, 2023b.\nOpenAI. Hello GPT-4o, 2024. URL https : / /openai . com/index/hello-gpt-4o/.\nOpenAI. Introducing ChatGPT, 2022. URL https : / /openai . com/ index/chatgpt /.\n",
        "page_info": {
            "page_no": 22,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "100\\mathbf{B}+\n=\n+\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A\nbenchmark. CoRR, abs/2311.12022, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\n adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99-106, 2021.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-\n mar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts\ninference and training to power next-generation AI scale. In ICML, volume 162 of Proceedings of\nMachine Learning Research,pp. 18332-18346.PMLR, 2022.\n24\n Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\n Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-\nBench tasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003-13051.\nAssociation for Computational Linguistics, 2023.\nURL https://spaces.ac.cn/archives/9577.\nbetter length extrapolation, 2023.\nJianlin Su. The magical effect of the Bias term: RoPE\nBias\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language\nm0dels. CoRR, abs/2302.13971, 2023.\nQwen Team. Introducing Qwenl.5, 2024a. URL https: //qwenlm.github.io/blog/\nqwen1.5/.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Mlia Polosukhin. Attention is all you need. In NIPS, pp. 5998-6008, 2017.\n//qwenlm.github.io/blog/qwenl.5-110b/.\nQwen Team. Qwen1.5-110B: The first\nmodel of the Qwen1.5 series, 2024b. URL https :\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. In NeurlPS,\n2023.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task\nlanguage understanding benchmark. CoRR, abs/2406.01574, 2024.\nQwen Team. Qwen1.5-MoE: Matching 7B model performance with 1/3 activated parameters, 2024c.\nURL https: / /qwenlm.github.io/blog/qwen-moe/.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng\n Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang,\nShiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng\nNie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\nYi: Open foundation models by 01.AI. CoRR, abs/2403.04652, 2024.\nQwen Team. Introducing Qwenl.5, 2024a. URL https: / /qwenlm.github.io/blog/\nqwen1.5/.\n//qwenlm.github.io/blog/qwen1.5-110b/.\nQwen Team. Qwen1.5-MoE: Matching 7B model performance with 1/3 activated parameters, 2024c.\nURL https: / /qwenlm.github.io/blog/qwen-moe/.\n Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea\n2023.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-\nmar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts\ninference and training to power next-generation AI scale. In ICML, volume 162 of Proceedings of\nMachine Learning Research,pp. 18332-18346. PMLR, 2022.\n David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A\nbenchmark. CoRR, abs/2311.12022, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nadversarial winograd schema challenge at scale. Commun. ACM, 64(9):99-106, 2021.\nURL https: / /spaces.ac.cn/archives/9577.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nMirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung.\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-\nBench tasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003-13051.\nAssociation for Computational Linguistics, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée\nLacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998-6008, 2017.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\n Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task\nlanguage understanding benchmark. CoRR, abs/2406.01574, 2024.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajwal Bhargava, Rui Hou, Louis Martin,\nRashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar\nMehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike\nLewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. CoRR,\nabs/2309.16039, 2023.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial\ndataset for paraphrase identification. In EMNLP/IJCNLP (1), pp. 3685-3690. Association for\nComputational Linguistics, 2019.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng\nZhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang\nShiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng\nNie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\nYi: Open foundation models by 01.AI. CoRR, abs/2403.04652, 2024.\nQwen Team. Qwen1.5-110B: The first\nmodel of the Qwen1.5 series, 2024b. URL https :\nbetter length extrapolation, 2023.\nJianlin Su. The magical effect of the Bias term: RoPE\nBias\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajwal Bhargava, Rui Hou, Louis Martin,\nRashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar\nMehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike\n Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. CoRR,\nabs/2309.16039,2023.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial\ndataset for paraphrase identification. In EMNLP/IJCNLP (1), pp. 3685-3690. Association for\nComputational Linguistics, 2019.\n",
        "page_info": {
            "page_no": 23,
            "height": 2200,
            "width": 1700
        }
    },
    {
        "layout_dets": "Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu\nYao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. LV-Eval: A balanced\nlong-context benchmark with 5 length levels up to 256K. CoRR, abs/2402.05136, 2024.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling re-\nlationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825,\n2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? In ACL (1), pp. 4791-4800. Association for Computational Linguistics,\n2019.\nAohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin\n Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie\nTang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang,\nPeng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang,\nWeng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan\nLiu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao\nYang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du,\n Zhenyu Hou, and Zihan Wang. ChatGLM: A family of large language models from GLM-130B to\nGLM-4 all t0ols. CoRR, abs/2406.12793, 2024.\nYingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Minghao Li, Fei Huang, Nevin L. Zhang, and\n Yongbin Li. Tree-Instruct: A preliminary study of the intrinsic relationship between complexity\nand alignment. In LREC/COLING, pp. 16776-16789. ELRA and ICCL, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging LLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS, 2023.\n Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,\nand Le Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911,\n2023.\n25\n",
        "page_info": {
            "page_no": 24,
            "height": 2200,
            "width": 1700
        }
    }
]